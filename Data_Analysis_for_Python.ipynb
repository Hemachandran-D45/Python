{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV2LZcBjz9RDmEprZITU2m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemachandran-D45/Python/blob/main/Data_Analysis_for_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Rfv2IjiYZ4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv\"\n",
        "df = pd.read_csv(url, header = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df #for print entire dataframe but not recommented for large dataset\n",
        "df.head(n) #print first n rows\n",
        "df.tail(n) #print last n rows"
      ],
      "metadata": {
        "id": "XbWtvh7KjBgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Save the file"
      ],
      "metadata": {
        "id": "yBoNPOiIjlO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"path for where to save file\"\n",
        "df.to_csv(path)"
      ],
      "metadata": {
        "id": "wBkdCnfSjrtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.dtypes #used to check the data type in Pandas"
      ],
      "metadata": {
        "id": "4dtrGGTq8MgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.describe() #used to return statistcal summary"
      ],
      "metadata": {
        "id": "nhNUkjk378CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.describe(include = \"all\") #used to Provide full summary"
      ],
      "metadata": {
        "id": "XsQMYBIF8gC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with Missing Data\n",
        "\n",
        "    Check with data collection source\n",
        "    Drop the missing value\n",
        "    Replace the missing value"
      ],
      "metadata": {
        "id": "edzMgzIOD2hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop\n",
        "df.dropna() #drop the missing data\n",
        "df.dropna(axis=0) #axis = 0 drop entire row, 1 for entire column"
      ],
      "metadata": {
        "id": "aNDZxoP7D5GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset = [\"column name\"],axis=0,inplace = True) #drop missing data in specific row and Inplace value true allow to modify data directly\n"
      ],
      "metadata": {
        "id": "PJUvPy_6ETHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace data with mean value\n",
        "\n",
        "mean = df[\"column name\"].mean()\n",
        "df[\"column name\"].replace(np.nan,mean)"
      ],
      "metadata": {
        "id": "KLh4BVyYEws4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace NaN in \"stroke\" column with the mean value."
      ],
      "metadata": {
        "id": "8veXzlyu0K7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "avg_stroke = df[\"stroke\"].astype(\"float\").mean(axis=0)\n",
        "df[\"stroke\"].replace(np.nan, avg_stroke, inplace=True)"
      ],
      "metadata": {
        "id": "EIcFD6eKz9lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Calculation to the entire column\n",
        "#convert \"mpg\" to L/100km by mathematical operation (235 divided by mpg)\n",
        "df[\"city-mpg\"] = 235/df[\"city-mpg\"]\n",
        "df.rename(columns = {\"city-mpg\":\"city-L/100km\"},inplace = True) #change the column name"
      ],
      "metadata": {
        "id": "hYXSkacHH1DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the example above, transform mpg to L/100km in the column of \"highway-mpg\" and change the name of column to \"highway-L/100km\"."
      ],
      "metadata": {
        "id": "1AJYysqs0JVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"highway-mpg\"] = 235/df[\"highway-mpg\"]\n",
        "df.rename(columns={'highway-mpg':'highway-L/100km'}, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xBJJGIuA0S8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correcting Datatype\n",
        "#Identify the datatype\n",
        "df.dtypes()\n",
        "#convert data type\n",
        "df.astype()\n",
        "\n",
        "#Eg:\n",
        "df[[\"bore\",\"stroke\"]] = df[[\"bore\",\"stroke\"]].astype(\"float\")\n",
        "df[[\"normalized-losses\"]] = df[[\"normalized-losses\"]].astype(\"int\")"
      ],
      "metadata": {
        "id": "7Gx4UhO9IJv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Normalization\n",
        "The process of organizing data in a structured format to make it easier to work with analyse\n",
        "\n",
        "There are three method"
      ],
      "metadata": {
        "id": "FBvCRq7UBHnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple Feature Scaling\n",
        "Xnew = Xold/Xmax\n",
        "df[\"length\"] = df[\"length\"]/df[\"length\"].max()\n",
        "\n"
      ],
      "metadata": {
        "id": "tmKM1d0iAoJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Min-Max\n",
        "Xnew = (Xold-Xmin)/(Xmax-Xmin)\n",
        "df[\"length\"] = (df[\"length\"]-df[\"length\"].min())/(df[\"length\"].max()-df[\"length\"].min())"
      ],
      "metadata": {
        "id": "qBSQ3rKLBmQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Z-Score\n",
        "Xnew = (Xold-mean)/std\n",
        "df[\"length\"] = (df[\"length\"]-df[\"length\"].mean())/df[\"length\"].std()"
      ],
      "metadata": {
        "id": "CcgPLa2qBwzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binning\n",
        "Simply called Bucketing , It is the preprocessing technique that group numerical value into bins or buck"
      ],
      "metadata": {
        "id": "OEI49uyRhaQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = np.linspace(min(df[\"price\"],max(df['price'])), 4)\n",
        "group_names = [\"Low\",\"Medium\",\"High\"]\n",
        "df[\"price-binned\"] = pd.cut(df[\"price\"],bins,labels = group_names,include_lowest = True)"
      ],
      "metadata": {
        "id": "8nO2Msihhx4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Turning Categorial Variables to Quantitative Variable\n",
        "\"Most of the model cannot take object, string as input \"\n",
        "    #Use dummy variable for each unique category and Assign 0 or 1 in each category\n",
        "\n",
        "#using One hot Encoding and use pandas.get_dummies() method\n",
        "\n",
        "pd.get_dummies(df[\"fuel\"])\n"
      ],
      "metadata": {
        "id": "neNCvSBIl8so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see which values are present in a particular column, we can use the \".value_counts()\" method:"
      ],
      "metadata": {
        "id": "pk0_HnxKyA1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"no_of_doors\"].value_counts()\n"
      ],
      "metadata": {
        "id": "ZTK9JumRxbbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that four doors are the most common type. We can also use the \".idxmax()\" method to calculate the most common type automatically:"
      ],
      "metadata": {
        "id": "SMho6I-fyKc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"no_of_doors\"].value_counts().idxmax()"
      ],
      "metadata": {
        "id": "i5hcWSt1yK8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA\n",
        "    Descriptive Statistics\n",
        "    Group BY\n",
        "    ANOVA - Analysis of Variance\n",
        "    Correlation\n",
        "    Correlation - Statistics"
      ],
      "metadata": {
        "id": "VVsNiyiZ0_A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Descriptive Statistics\n",
        "df.describe()\n",
        "#its show mean, total number of data points, std etc"
      ],
      "metadata": {
        "id": "w-dlN6EU1HRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Summarizing the categorical data\n",
        "df.value_counts()\n",
        "\n",
        "#Eg\n",
        "drive_wheels_counts = df['drive-wheels'].value_counts().to_frame()\n",
        "drive_wheels_counts.rename(columns={'drive-wheels': 'value_counts'}, inplace=True)\n",
        "drive_wheels_counts.index.name = 'drive-wheels'\n"
      ],
      "metadata": {
        "id": "EVg8_nEg1KZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Group By method\n",
        "dataframe.groupby()  #group a data"
      ],
      "metadata": {
        "id": "-GVio2B71MUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df[[\"drive-wheel\",'body-style','price']]\n",
        "df_grp = df_test.groupby([\"drive-wheel\"],as_index = False).mean() # false as index for not group bu index\n",
        "df_grp"
      ],
      "metadata": {
        "id": "97pa4VNK2bnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A table of thos form isn't the easiest to read and also not easy to visualize\n"
      ],
      "metadata": {
        "id": "y0orXnXD200J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use pivot method\n",
        "df_pivot = df_grp.pivot(index = 'drive-wheel',columns = 'body-style')\n",
        "df_pivot\n",
        "\n",
        "#Pivot() one variable display along the colum and the other variable displayed along the row"
      ],
      "metadata": {
        "id": "--xySryU2y3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heatmap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.pcolor(df_pivot,cmap = 'RdBu')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ly9efFfW3IX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA - Analysis of Variance\n",
        "\n",
        "    Statistical comparison of group\n",
        "    Eg: Average price of different vehicle maker\n",
        "\n",
        "Why we perform ?\n",
        "\n",
        "    It show correlation between two groups\n",
        "\n",
        "What we obtain?\n",
        "\n",
        "    F-test Score- variation between sample group mean divided by variation between sample group\n",
        "    p-value:Confidence degree\n"
      ],
      "metadata": {
        "id": "pzf2GUVE4DWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ANOVA between Honda and Subaru\n",
        "\n",
        "df_anova = df[[\"make\",\"price\"]]\n",
        "grouped_anova = df_anova.groupby([\"make\"])\n",
        "anova_results_1 = stats.f_oneway(grouped_anova.get_group(\"honda\")[\"price\"],grouped_anova.get_group(\"subaru\")[\"price\"])\n",
        "#oneway method is function in scipy packages. The oinewa anova tests the NULL hypothesis that two or more groups have a same population.\n",
        "ANOVA results: F= 0.19744031275 , p = F_onewayResults(statistic=0.1974403127), pvalues=0.66094"
      ],
      "metadata": {
        "id": "ATn05XJJ5lr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anova_results_2 = stats.f_oneway(grouped_anova.get_group(\"honda\")[\"price\"],grouped_anova.get_group(\"jaguar\")[\"prices\"])\n"
      ],
      "metadata": {
        "id": "o8b9EFsi7r2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation\n",
        "\n",
        "    Measures to what extent different variablea are interdependent(if one variable changes, how does other variable changes)\n",
        "    Rain---Umbrella , Lung cancer ---- smoking\n",
        "\n",
        "    Correlation doesn't imply causation(we cant tell whether umbrella cause the rain or rain caused the umbrella)\n"
      ],
      "metadata": {
        "id": "vgPbDe0MNBev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets see correlation between engine size and prices\n",
        "sns.regplot(x = \"engine-size\",y = \"price\",data = df)\n",
        "plt.ylim(0,)\n",
        "#positive linear regression"
      ],
      "metadata": {
        "id": "dyQfbfp4NBMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation between highway-mpg and prices\n",
        "sns.regplot(x = \"highway-mpg\",y = \"price\",data = df)\n",
        "plt.ylim(0,)\n",
        "#negative linear regression"
      ],
      "metadata": {
        "id": "Bb7cEAiITkgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Weak correlation between two features (peak-rpm, prices)\n",
        "sns.regplot(x = \"peak-rpm\",y = \"price\",data = df)\n",
        "plt.ylim(0,)"
      ],
      "metadata": {
        "id": "-tvrkjjYUgkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Statistical method\n",
        "\n",
        "  Pearson Correlation\n",
        "\n",
        "      Measure the strength of the correlation between two feature\n",
        "      1)correlation coefficient\n",
        "      2)p-value\n"
      ],
      "metadata": {
        "id": "WVU8UEEvU3L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_coef, p_value = stats.pearsonr(df['horsepower'], df['price'])\n",
        "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)"
      ],
      "metadata": {
        "id": "A4dJXLXmV1cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Development\n",
        "\n",
        "    Simple and Multiple Linear Regression\n",
        "    Model Evalution using Visualization\n",
        "    Polynomial Regression and Pipelines\n",
        "    R-square and MSE for in sample Evalution\n",
        "    Prediction and Decision making\n",
        "\n",
        "Question:\n",
        "  How do you find the fair value of used car ?\n",
        "\n"
      ],
      "metadata": {
        "id": "jFCzykv_z25k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression:\n",
        "\n",
        "Linear Regression will refer to one independent variable to make prediction\n",
        "\n",
        "    It is the relation between predictor(independent) x and the target(dependent) y\n",
        "\n"
      ],
      "metadata": {
        "id": "vZLWq5Su0-5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple linear regression model\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lm = LinearRegression() #constructor\n",
        "x =  df[['highway-mpg']]\n",
        "y = df[['price']]\n",
        "lm.fit(x,y) #fitting the model\n",
        "Yhat = lm.predict(x) #predicting the model\n",
        "#price = intercep - coef * highway-mpg"
      ],
      "metadata": {
        "id": "9PI7iLqp8YQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Multiple Linear Regression:\n",
        "\n",
        "Multiple indiependent variable to make prediction\n",
        "\n",
        "    replation between one continuous target y and twp or more predict variable x"
      ],
      "metadata": {
        "id": "ERktkRvU8XtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiple Linear Regression\n",
        "\n",
        "z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']] #four predictor value\n",
        "lm.fit(z, df['price'])\n",
        "Yhat = lm.predict(z)\n"
      ],
      "metadata": {
        "id": "xsN--vE11_Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evalution using Visualization\n",
        "\n",
        "Regression Plot\n",
        "   \n",
        "    Why we use ?\n",
        "      The relation between two variable\n",
        "      Te strength of the correlation\n",
        "      the direction of the relationship (+ or -)"
      ],
      "metadata": {
        "id": "QNHs2N5JATV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple way to use regplot\n",
        "\n",
        "import seaborn as sns\n",
        "sns.regplot(x = \"highway-mpg\",y = \"price\",data = df) #x independent variable  y dependent\n",
        "plt.ylim(0,)\n",
        "\n",
        "#residual plot represent the actual value"
      ],
      "metadata": {
        "id": "9P1f_SZYCZtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#residual plot\n",
        "\n",
        "import seaborn as sns\n",
        "sns.residplot(df['highway-mpg'],df['price'])"
      ],
      "metadata": {
        "id": "5wamF1nrJjIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot\n",
        "\n",
        "ax1 = sns.distplot(df['price'],hist = False,color = \"r\",label = \"Actual Value\") # dont need histogram so false\n",
        "sns.distplot(Yhat,hist = False,color = \"b\",label = \"Fitted Values\",ax = ax1)"
      ],
      "metadata": {
        "id": "FwwkZv4AKuFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression:\n",
        "\n",
        "    A special case of general linear regression model\n",
        "    Useful for describing curvilinear relationships\n",
        "\n",
        "    Curvilinear Relationship:\n",
        "    By squaring or setting higher-order terms of the predictor variables in the model, transforming the data"
      ],
      "metadata": {
        "id": "6yHAZj4jLojB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating 3rd order polynomial regression\n",
        "f = np.polyfit(x,y,3)\n",
        "p = np.poly1d(f)\n",
        "print(p)\n",
        "\n",
        "\n",
        "#multi-dimensional polynomial linear regression\n",
        "#if the expression is complicate polyfit function wont work so use that PREPROCESSING library from scikit learn\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "pr = PolynomialFeatures(degree = 2)\n",
        "pr.fit_transform([1,2],include_bias = False)\n",
        "\n",
        "#As the dimension of data get larger we may want to normalize the feature\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Srismk4NNbo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "SCALE = StandardScaler()\n",
        "SCALE.fit(df[['horsepower','highway-mpg']])\n",
        "x_scale = SCALE.transform(df[['horsepower','highway-mpg']])\n",
        "\n",
        "#Pipelines\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "Input = [\n",
        "          ('scale',StandardScaler()),\n",
        "          ('polynomial',PolynomialFeatures(include_bias = False)),\n",
        "          ('model',LinearRegression())\n",
        "        ]\n",
        "#the first element in the tuple contains the name of the estimator : model , second: model constructor\n",
        "pipe = Pipeline(Input) #pipeline object\n",
        "X=df[[\"horsepower\",\"curb-weight\",\"engine-size\",\"highway-mpg\"]]\n",
        "Y = df[\"price\"]\n",
        "\n",
        "pipe.fit(X,Y)#we can train the pipeline object\n",
        "Yhat = pipe.predict(X) #we can make the prediction"
      ],
      "metadata": {
        "id": "P6ZfvIbfN-yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure for In Sample Evaluate\n",
        "\n",
        "    Two important method\n",
        "        Mean Square Error(MSE)\n",
        "        R-Square\n",
        "\n",
        "    Mean Square Error:\n",
        "        we find the different between the actual value y and predicted value yhat then square it\n",
        "\n",
        "    R-Square\n",
        "        Also called coefficient of determination\n",
        "        determine how close the data is fitted regression line\n",
        "        The percentage of variance of the target variable (Y) that is explained by the linear model\n",
        "        "
      ],
      "metadata": {
        "id": "-I1Cio_VbZ7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metric import mean_squared_error\n",
        "mean_squared_error(df['price'],Yhat)"
      ],
      "metadata": {
        "id": "DKgB6Fx7c34B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction and Decision Making\n",
        "\n",
        "    To determine best fit model ,\n",
        "      Do the predicted value make sense\n",
        "      Visualization\n",
        "      Numerical measures of evalution\n",
        "      Compare models"
      ],
      "metadata": {
        "id": "CUJFRauscx0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First we train the model\n",
        "x = df[['highway-mpg']]\n",
        "y = df[['price']]\n",
        "lm.fit(x,y)\n",
        "#let predict price of care with 30 highwaympg\n",
        "lm.predict(np.array(30).reshape(-1,1))\n",
        "\n",
        "#Result like 13k dollor\n",
        "\n",
        "#for range of 1 to 100 h-mpg\n",
        "import numpy as np\n",
        "new_input = np.arange(1,101,1).reshape(-1,1)\n",
        "new_output = lm.predict(new_input)\n"
      ],
      "metadata": {
        "id": "jxsDPGBwf2yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation and Refinement\n",
        "\n",
        "    Model Evaluation\n",
        "    Over-fitting, Under Fitting, and Model Selection\n",
        "    Ridge Regression\n",
        "    Grid Search\n"
      ],
      "metadata": {
        "id": "PZCW7cRflNKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size = 0.3,random_state = 0)\n",
        "#x_data : independent variable\n",
        "#y_data : target variable (price)\n",
        "#x_train , y_train : part of training set\n",
        "#x_test, y_test : part of testing set\n",
        "#test size\n",
        "#random state : number generator for random sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "4Jp1FNDslp8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generalization Error\n",
        "\n",
        "    Generalization error is a measure of how well our data does at predicting previously unseen data\n",
        "\n",
        "    The error we obtain using our testing data is an approximation of this error.\n",
        "\n",
        "Cross Validation\n",
        "\n",
        "    Most common out of sample evaluation metrics"
      ],
      "metadata": {
        "id": "Ty-aFw2htkS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(lr,x_data,y_data,cv = 3) #first : type of model\n",
        "np.mean(scores)\n",
        "\n",
        "#cross validation predict\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "yhat = cross_val_predict(lr2e,x_data,y_data,cv = 3)\n"
      ],
      "metadata": {
        "id": "aZsEBmd6qivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Selection\n",
        "\n",
        "rsq = []\n",
        "order = [1,2,3,4]\n",
        "for n in order:\n",
        "  pr = PolynomialFeatures(degree = n)\n",
        "  x_train = pr.fit_transform(df[['horsepower']])\n",
        "  x_test = pr.fit_transform(df_test[['horsepower']])\n",
        "  lr.fit(x_train,pr,y_train)\n",
        "  rsq.append(lr.score(x_test,pr,y_test))\n",
        "\n",
        "plt.plot(order,rsq)"
      ],
      "metadata": {
        "id": "7v_rTV-o_5Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression: Prevent over fitting , control the magnitude of the polynomial coefficient by introducing ALPHA.\n",
        "\n",
        "\n",
        "Alpha is a parameter we select before fitting or training the model."
      ],
      "metadata": {
        "id": "zJaNPBw6BJDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "RidgeModel = Ridge(alpha = 0.1)\n",
        "RidgeModel.fit(x,y)\n",
        "Yhat = RidgeModel.predict(x)"
      ],
      "metadata": {
        "id": "Sj33TjpXEo_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search\n",
        "\n",
        "    Hyperparameters\n",
        "        The term alpha in ridge regression is called hyperparameter here.\n",
        "        Scikit-learn has a means of automatically iterating over these hyperparameters using\n",
        "        cross-validation.\n",
        "\n",
        "Grid Search take - scoring method(r square value), no. of folds, model or object(Ridge), free parameter value"
      ],
      "metadata": {
        "id": "hZTcPghXGDXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameter1 = [{'alpha': [0.001,0.01.0.1,0.1,1,10,100,1000,10000]}]\n",
        "RR = Ridge()\n",
        "Grid1 = GridSearchCV(RR,parameter1,cv = 4)\n",
        "Grid1.fit(df[['horsepower','curb-weight','engine-size','highway-mpg']],df[['price']])\n",
        "Grid1.best_estimator_\n",
        "\n",
        "scores = Grid1.cv_results_\n",
        "scores[\"mean_test_score\"]"
      ],
      "metadata": {
        "id": "c8YXVtq7I20n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}